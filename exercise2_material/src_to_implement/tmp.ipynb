{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "class car:\n",
    "    __brand =None\n",
    "    def __init__(self,color,brand):\n",
    "        self.color = color\n",
    "        self.__brand = brand\n",
    "    def ss(self):\n",
    "        print(self.__brand)\n",
    "       \n",
    "c = car('blue','BMW')\n",
    "print(c.color)\n",
    "print(c.ss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "s=np.array([1,1,1])\n",
    "s2=np.array([[2,7,2],[2,2,2]])\n",
    "np.unravel_index(np.argmax(s2), s2.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class L2Loss:\n",
    "    def __init__(self):\n",
    "        self.input_tensor = None\n",
    "    def forward(self, input_tensor, label_tensor):\n",
    "        self.input_tensor = input_tensor\n",
    "        return np.sum(np.square(input_tensor - label_tensor))\n",
    "    def backward(self, label_tensor):\n",
    "        return 2*np.subtract(self.input_tensor, label_tensor)\n",
    "def gradient_check(layers, input_tensor, label_tensor):\n",
    "    epsilon = 1e-5\n",
    "    difference = np.zeros_like(input_tensor)\n",
    "\n",
    "    activation_tensor = input_tensor.copy()\n",
    "    for layer in layers[:-1]:\n",
    "        activation_tensor = layer.forward(activation_tensor)\n",
    "    layers[-1].forward(activation_tensor, label_tensor)\n",
    "\n",
    "    error_tensor = layers[-1].backward(label_tensor)\n",
    "    for layer in reversed(layers[:-1]):\n",
    "        error_tensor = layer.backward(error_tensor)\n",
    "\n",
    "    it = np.nditer(input_tensor, flags=['multi_index'])\n",
    "    while not it.finished:\n",
    "        plus_epsilon = input_tensor.copy()\n",
    "        plus_epsilon[it.multi_index] += epsilon\n",
    "        minus_epsilon = input_tensor.copy()\n",
    "        minus_epsilon[it.multi_index] -= epsilon\n",
    "\n",
    "        analytical_derivative = error_tensor[it.multi_index]\n",
    "\n",
    "        for layer in layers[:-1]:\n",
    "            plus_epsilon = layer.forward(plus_epsilon)\n",
    "            minus_epsilon = layer.forward(minus_epsilon)\n",
    "        upper_error = layers[-1].forward(plus_epsilon, label_tensor)\n",
    "        lower_error = layers[-1].forward(minus_epsilon, label_tensor)\n",
    "\n",
    "        numerical_derivative = (upper_error - lower_error) / (2 * epsilon)\n",
    "\n",
    "        # print('Analytical: ' + str(analytical_derivative) + ' vs Numerical :' + str(numerical_derivative))\n",
    "        normalizing_constant = max(np.abs(analytical_derivative), np.abs(numerical_derivative))\n",
    "\n",
    "        if normalizing_constant < 1e-15:\n",
    "            difference[it.multi_index] = 0\n",
    "        else:\n",
    "            difference[it.multi_index] = np.abs(analytical_derivative - numerical_derivative) / normalizing_constant\n",
    "\n",
    "        it.iternext()\n",
    "    return difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from Layers.Base import BaseLayer\n",
    "class Conv(BaseLayer):\n",
    "    def __init__(self, stride_shape, convolution_shape, num_kernel):\n",
    "        self.trainable = True\n",
    "        self.kernel_shape = (num_kernel, *convolution_shape)\n",
    "        self.weights = np.random.uniform(0, 1, self.kernel_shape)\n",
    "        self.bias = np.random.uniform(0, 1, (num_kernel, 1)) #(4,1)\n",
    "        self.stride = stride_shape\n",
    "        self.num_kernel = num_kernel\n",
    "        self.convolution_shape = convolution_shape\n",
    "        self.gw = None\n",
    "        self.gb = None\n",
    "        self.optimizer =None\n",
    "        \n",
    "\n",
    "    @property\n",
    "    def gradient_weights(self):\n",
    "        return self.gw\n",
    "\n",
    "    @property\n",
    "    def gradient_bias(self):\n",
    "        return self.gb\n",
    "\n",
    "    def initialize(self,weights_initializer_methods, bias_initializer_methods):\n",
    "        fan_in = math.prod(self.convolution_shape)\n",
    "        fan_out = fan_in / self.convolution_shape[0] * self.num_kernel\n",
    "        self.weights = weights_initializer_methods.initialize(self.weights.shape,fan_in,fan_out)\n",
    "        self.bias = bias_initializer_methods.initialize(self.bias.shape,fan_in,fan_out)\n",
    "\n",
    "    def conv_1D(self, input, output_shape,stride):\n",
    "        \"\"\"\n",
    "            input: (c, y)\n",
    "            output_shape: (num_kernel, (y-m)/s)\n",
    "            output: (num_kernel, (y-m)/s)\n",
    "        \"\"\"\n",
    "        num_kernel, output_width = output_shape\n",
    "        output = np.zeros(output_shape)\n",
    "\n",
    "        for k in range(num_kernel):\n",
    "            kernel = self.weights[k]\n",
    "            bias = self.bias[k]\n",
    "            for i in range(0, input.shape[-1] - kernel.shape[-1] + 1, stride):\n",
    "                region = input[:, i:i + kernel.shape[-1]]\n",
    "                output[k, i // stride] = np.sum(region * kernel) + bias\n",
    "\n",
    "        return output\n",
    "\n",
    "    def conv_2D(self, input, output_shape, strides):\n",
    "        \"\"\"\n",
    "            input: (c, y, x)\n",
    "            output_shape: (num_kernel, (y-m)/s_y, (x-n)/s_x)\n",
    "            strides: (stride_y, stride_x)\n",
    "            kernel: (num_kernel, c, y, x)\n",
    "            output: (num_kernel, (y-m)/s_y, (x-n)/s_x)\n",
    "        \"\"\"\n",
    "        stride_y, stride_x = strides\n",
    "        num_kernel, output_height, output_width = output_shape\n",
    "        output = np.zeros(output_shape)\n",
    "\n",
    "        for k in range(num_kernel):\n",
    "            kernel = self.weights[k]\n",
    "            bias = self.bias[k]\n",
    "            for i in range(0, input.shape[1] - kernel.shape[1]+1, stride_y):\n",
    "                for j in range(0, input.shape[2] - kernel.shape[2]+1, stride_x):\n",
    "                    region = input[:, i:i + kernel.shape[1], j:j + kernel.shape[2]]\n",
    "                    output[k, i // stride_y, j // stride_x] = np.sum(region * kernel) + bias\n",
    "        return output\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        \"\"\"\n",
    "        input_tensor: (b, c, y) / (b, c, y, x)\n",
    "        output_tensor: (b, num_kernel, (y-m)//s+1) /(b, num_kernel, (y-m)//s_y+1, (x-n)//s_x+1)\n",
    "        \"\"\"\n",
    "        n = len(input_tensor.shape)\n",
    "        output = []\n",
    "        self.input_tensor = input_tensor\n",
    "        if n == 3:\n",
    "            # 1D convolution\n",
    "            b, c, y = input_tensor.shape\n",
    "            py = (self.kernel_shape[2]-1)//2\n",
    "            py_end = py\n",
    "            if self.kernel_shape[2]%2 ==0 :\n",
    "                    py_end +=1\n",
    "            self.padding = [(0,0),  (0,0), (py,py_end)]\n",
    "            self.pad_input_tensor = np.pad(input_tensor, self.padding, mode='constant', constant_values=0)\n",
    "            output_shape = (self.num_kernel, (self.pad_input_tensor.shape[-1] - self.kernel_shape[2]) // self.stride[0]+1)\n",
    "            for i in range(b):\n",
    "                single_input = self.pad_input_tensor[i]\n",
    "                output.append(self.conv_1D(single_input, output_shape,self.stride[0]))\n",
    "        elif n == 4:\n",
    "            # 2D convolution\n",
    "            b, c, y, x = input_tensor.shape\n",
    "            if isinstance(self.stride, tuple):\n",
    "                self.stride_y, self.stride_x = self.stride\n",
    "            else:\n",
    "                self.stride_y, self.stride_x = self.stride, self.stride\n",
    "            px,py = (self.kernel_shape[3]-1)//2,(self.kernel_shape[2]-1)//2\n",
    "            px_end,py_end = px,py\n",
    "            if self.kernel_shape[3]%2 ==0 :\n",
    "                    px_end +=1\n",
    "            if  self.kernel_shape[2]%2 ==0 :\n",
    "                    py_end +=1\n",
    "            self.padding = [(0, 0), (0, 0), (py, py_end), (px, px_end)]\n",
    "            self.pad_input_tensor = np.pad(input_tensor, self.padding, mode='constant', constant_values=0)\n",
    "\n",
    "            output_shape = (self.num_kernel,\n",
    "                            (self.pad_input_tensor.shape[-2]-self.weights.shape[-2])//self.stride_y+1,\n",
    "                            (self.pad_input_tensor.shape[-1]-self.weights.shape[-1])//self.stride_x+1)\n",
    "            for i in range(b):\n",
    "                single_input = self.pad_input_tensor[i]\n",
    "                output.append(self.conv_2D(single_input, output_shape, [self.stride_y, self.stride_x]))\n",
    "\n",
    "        self.output = np.array(output)\n",
    "        return self.output\n",
    "    \n",
    "    def backward_1D(self, error_tensor):\n",
    "        db = np.sum(error_tensor, axis=(0,2)).reshape(-1,1)\n",
    "        dw = np.zeros(self.weights.shape)\n",
    "        padded_err = np.pad(error_tensor, [(0, 0), (0, 0), (self.kernel_shape[-1] - 1, self.kernel_shape[-1] - 1)], mode='constant', constant_values=0)\n",
    "        for k in range(self.num_kernel):\n",
    "            for i in range(0,(self.pad_input_tensor.shape[-1]-error_tensor.shape[-1] )//self.stride[0]-1):\n",
    "                dw[k,:,i] = np.sum(self.pad_input_tensor[:,:,i:i+error_tensor.shape[2]] * error_tensor[:,k:k+1,:],axis=(0,2))\n",
    "        self.gb = db\n",
    "        self.gw = dw\n",
    "        d_input = np.zeros(self.input_tensor.shape)#(b,c,w)\n",
    "        flipped_weights=np.flip(self.weights, axis=-1)\n",
    "        for n in range(self.input_tensor.shape[0]):  # 遍历批次\n",
    "            for c_in in range(self.input_tensor.shape[1]):  # 遍历输入通道\n",
    "                for c_out in range(self.weights.shape[0]):  # 遍历输出通道\n",
    "                    # 卷积核与步幅处理\n",
    "                    flipped_kernel = flipped_weights[c_out, c_in, :]\n",
    "                    for i in range(error_tensor.shape[-1]):  # 按照步幅更新 d_input\n",
    "                        start = i * self.stride[0]\n",
    "                        end = start + self.weights.shape[-1]\n",
    "                        if end <= self.input_tensor.shape[-1]:  # 确保范围合法\n",
    "                            d_input[n, c_in, start:end] += error_tensor[n, c_out, i] * flipped_kernel\n",
    "        return d_input\n",
    "    \n",
    "    def conv2d_backward(self,error_tensor, input_tensor, kernel, stride_y, stride_x, padding):\n",
    "        # 获取各种维度信息\n",
    "        batch_size, in_channel, input_height, input_width = input_tensor.shape\n",
    "        num_kernels, _, kernel_height, kernel_width = kernel.shape\n",
    "\n",
    "        # 计算填充后的输入维度\n",
    "        padded_height = input_height + 2 * padding\n",
    "        padded_width = input_width + 2 * padding\n",
    "        pad_input_tensor = np.pad(input_tensor, ((0, 0), (0, 0), (padding, padding), (padding, padding)), 'constant')\n",
    "\n",
    "        # 初始化梯度张量\n",
    "        grad_input = np.zeros_like(pad_input_tensor)\n",
    "\n",
    "        # 反向传播计算梯度\n",
    "        for n in range(batch_size):\n",
    "            for k in range(num_kernels):\n",
    "                for c in range(in_channel):\n",
    "                    for i in range(error_tensor.shape[2]):\n",
    "                        for j in range(error_tensor.shape[3]):\n",
    "                            h_start = i * stride_y\n",
    "                            w_start = j * stride_x\n",
    "                            h_end = h_start + kernel_height\n",
    "                            w_end = w_start + kernel_width\n",
    "\n",
    "                            grad_input[n, c, h_start:h_end, w_start:w_end] += kernel[k, c, :, :] * error_tensor[n, k, i, j]\n",
    "\n",
    "        # 移除填充\n",
    "        if padding > 0:\n",
    "            grad_input = grad_input[:, :, padding:-padding, padding:-padding]\n",
    "\n",
    "        return grad_input\n",
    "\n",
    "    def backward_2D(self, error_tensor):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        db = np.sum(error_tensor, axis=(0,2,3)).reshape(-1,1)\n",
    "        dw = np.zeros_like(self.weights)\n",
    "        grad_input = np.zeros_like(self.pad_input_tensor)\n",
    "        rotated_kernels = np.flip(self.weights, axis=(2, 3))\n",
    "        # 卷积核梯度计算\n",
    "        for n in range(error_tensor.shape[0]):  # 遍历批次\n",
    "            for k in range(self.num_kernel):  # 遍历每个卷积核\n",
    "                for c in range(self.kernel_shape[1]):  # 遍历输入通道\n",
    "                    for i in range(error_tensor.shape[2]):  # 遍历输出高度\n",
    "                        for j in range(error_tensor.shape[3]):  # 遍历输出宽度\n",
    "                            start_i,end_i = i * self.stride_y,i * self.stride_y + self.kernel_shape[2]\n",
    "                            start_j,end_j = j * self.stride_x,j * self.stride_x + self.kernel_shape[3]\n",
    "\n",
    "                            if end_i <= self.pad_input_tensor.shape[2] and end_j <= self.pad_input_tensor.shape[3]:\n",
    "                                input_slice = self.pad_input_tensor[n, c, start_i:end_i, start_j:end_j]\n",
    "                                error_value = error_tensor[n, k, i, j]\n",
    "                                dw[k, c, :, :] += input_slice * error_value\n",
    "        \"\"\"\n",
    "        print(error_tensor.shape)\n",
    "        print(self.kernel_shape)\n",
    "        print(self.weights.shape)\n",
    "        print(self.pad_input_tensor.shape)\n",
    "        print(self.input_tensor.shape)\n",
    "        print(self.padding[3])\n",
    "        \"\"\"\n",
    "        # 输入梯度计算\n",
    "        padded_error = np.pad(\n",
    "            error_tensor,\n",
    "            [(0, 0), (0, 0), (self.kernel_shape[2] - 1, self.kernel_shape[2] - 1), (self.kernel_shape[3] - 1, self.kernel_shape[3] - 1)],\n",
    "            mode=\"constant\",\n",
    "            constant_values=0,\n",
    "        )\n",
    "        # 反向传播计算梯度\n",
    "        for n in range(error_tensor.shape[0]):\n",
    "            for k in range(self.num_kernel):\n",
    "                for c in range(self.kernel_shape[1]):\n",
    "                    for i in range(error_tensor.shape[2]):\n",
    "                        for j in range(error_tensor.shape[3]):\n",
    "                            h_start = i * self.stride_y\n",
    "                            w_start = j * self.stride_x\n",
    "                            h_end = h_start + self.kernel_shape[2]\n",
    "                            w_end = w_start + self.kernel_shape[3]\n",
    "\n",
    "                            grad_input[n, c, h_start:h_end, w_start:w_end] += self.weights[k, c, :, :] * error_tensor[n, k, i, j]\n",
    "        # 去除填充部分，恢复输入形状\n",
    "        py, py_end = self.padding[2]\n",
    "        px, px_end = self.padding[3]\n",
    "        grad_input = grad_input[:, :, py:-py_end if py_end != 0 else None, px:-px_end if px_end != 0 else None]\n",
    "        \n",
    "        #grad_input = self.conv2d_backward(error_tensor, self.input_tensor, self.weights, self.stride_y, self.stride_x, 1)\n",
    "        # 保存梯度\n",
    "        self.gb = db\n",
    "        self.gw = dw\n",
    "        return grad_input\n",
    "\n",
    "    def backward(self, error_tensor):\n",
    "        \"\"\"\n",
    "            error_tensor: (b, num_kernel, output_height, output_width) / (b, num_kernel, output_length)\n",
    "        \"\"\"\n",
    "        if len(self.input_tensor.shape) == 3:  # 1D convolution backward\n",
    "            gradin_input =  self.backward_1D(error_tensor)\n",
    "        elif len(self.input_tensor.shape) == 4:  # 2D convolution backward\n",
    "            gradin_input =  self.backward_2D(error_tensor)\n",
    "        if self.optimizer is not None:\n",
    "            self.weights = self.optimizer.calculate_update(self.weights,self.gw)\n",
    "            self.bias = self.optimizer.calculate_update(self.bias,self.gb)\n",
    "        return gradin_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104.87743662253294 286940.8973849519\n",
      "286940.8973849519 1303569929.1050885\n",
      "1303569929.1050885 6953521110109.354\n",
      "6953521110109.354 3.993451309537277e+16\n",
      "3.993451309537277e+16 2.3958568256211694e+20\n",
      "2.3958568256211694e+20 1.481805749509826e+24\n",
      "1.481805749509826e+24 9.382710787347455e+27\n",
      "9.382710787347455e+27 6.056300202804526e+31\n",
      "6.056300202804526e+31 3.9727468387244e+35\n",
      "3.9727468387244e+35 2.6419073227920248e+39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HBen_\\AppData\\Local\\Temp\\ipykernel_30208\\2690429956.py:68: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  output[k, i // stride_y, j // stride_x] = np.sum(region * kernel) + bias\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "from Optimization import Optimizers\n",
    "from Layers import Initializers\n",
    "\n",
    "class TestConv(unittest.TestCase):\n",
    "    plot = False\n",
    "    directory = 'plots/'\n",
    "\n",
    "    class TestInitializer:\n",
    "        def __init__(self):\n",
    "            self.fan_in = None\n",
    "            self.fan_out = None\n",
    "\n",
    "        def initialize(self, shape, fan_in, fan_out):\n",
    "            self.fan_in = fan_in\n",
    "            self.fan_out = fan_out\n",
    "            weights = np.zeros((1, 3, 3, 3))\n",
    "            weights[0, 1, 1, 1] = 1\n",
    "            return weights\n",
    "\n",
    "    def setUp(self):\n",
    "        self.batch_size = 2\n",
    "        self.input_shape = (3, 10, 14)\n",
    "        self.input_size = 14 * 10 * 3\n",
    "        self.uneven_input_shape = (3, 11, 15)\n",
    "        self.uneven_input_size = 15 * 11 * 3\n",
    "        self.spatial_input_shape = np.prod(self.input_shape[1:])\n",
    "        self.kernel_shape = (3, 5, 8)\n",
    "        self.num_kernels = 4\n",
    "        self.hidden_channels = 3\n",
    "\n",
    "        self.categories = 105\n",
    "        self.label_tensor = np.zeros([self.batch_size, self.categories])\n",
    "        for i in range(self.batch_size):\n",
    "            self.label_tensor[i, np.random.randint(0, self.categories)] = 1\n",
    "    def test_update(self):\n",
    "        input_tensor = np.random.uniform(-1, 1, (self.batch_size, *self.input_shape))\n",
    "        conv = Conv((3, 2), self.kernel_shape, self.num_kernels)\n",
    "        conv.optimizer = Optimizers.Sgd(1)\n",
    "        conv.initialize(Initializers.He(), Initializers.Constant(0.1))\n",
    "        # conv.weights = np.random.rand(4, 3, 5, 8)\n",
    "        # conv.bias = 0.1 * np.ones(4)\n",
    "        for _ in range(10):\n",
    "            output_tensor = conv.forward(input_tensor)\n",
    "            error_tensor = np.zeros_like(output_tensor)\n",
    "            error_tensor -= output_tensor\n",
    "            conv.backward(error_tensor)\n",
    "            new_output_tensor = conv.forward(input_tensor)\n",
    "            print(np.sum(np.power(output_tensor, 2)), np.sum(np.power(new_output_tensor, 2)))\n",
    "test = TestConv()\n",
    "test.setUp()\n",
    "test.test_update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "class NeuralNetwork():\n",
    "    def __init__(self,optimizer,Initial_weights_method,Initial_bias_method):\n",
    "        self.optimizer=optimizer\n",
    "        self.Initial_weights_method=Initial_weights_method\n",
    "        self.Initial_bias_method=Initial_bias_method\n",
    "        self.loss=[]\n",
    "        self.layers=[]\n",
    "        self.data_layer=None\n",
    "        self.loss_layer=None\n",
    "        self.label=None\n",
    "        self.output=None\n",
    "    def forward(self):\n",
    "        input_tensor,label_tensor=self.data_layer.next()\n",
    "        self.label=label_tensor\n",
    "        output_tensor = input_tensor.copy()\n",
    "        for layer in self.layers:\n",
    "            output_tensor = layer.forward(output_tensor)\n",
    "        self.output=self.loss_layer.forward(output_tensor,self.label)\n",
    "        return self.output\n",
    "    def backward(self):\n",
    "        error_tensor = self.loss_layer.backward(self.label)\n",
    "        for layer in reversed(self.layers):\n",
    "            error_tensor = layer.backward(error_tensor)\n",
    "    def append_layer(self,layer):\n",
    "        if layer.trainable:\n",
    "            layer.initialize(self.Initial_weights_method,self.Initial_bias_method)\n",
    "            layer.optimizer = copy.deepcopy(self.optimizer)\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def train(self,iterations):\n",
    "        for it in range(iterations):\n",
    "            self.loss.append(self.forward())\n",
    "            self.backward()\n",
    "    def test(self,input_tensor):\n",
    "        output_tensor = input_tensor.copy()\n",
    "        for layer in self.layers:\n",
    "            output_tensor = layer.forward(output_tensor)\n",
    "        return output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "(5, 3)\n",
      "(5, 3)\n",
      "Adam: _____ start(4, 3)\n",
      "Adam:(4, 3)\n",
      "(4, 3) (4, 3)\n",
      "(4, 3)\n",
      "(4, 3)\n",
      "(4, 3)\n",
      "Adam: _____ end(4, 3)\n",
      "Adam: _____ start(1, 3)\n",
      "Adam:(1, 3)\n",
      "(1, 3) (1, 3)\n",
      "(1, 3)\n",
      "(1, 3)\n",
      "(1, 3)\n",
      "Adam: _____ end(1, 3)\n",
      "-------------------------------\n",
      "(5, 3)\n",
      "(5, 3)\n",
      "Adam: _____ start(4, 3)\n",
      "Adam:(4, 3)\n",
      "(4, 3) (4, 3)\n",
      "(4, 3)\n",
      "(4, 3)\n",
      "(4, 3)\n",
      "Adam: _____ end(4, 3)\n",
      "Adam: _____ start(1, 3)\n",
      "Adam:(1, 3)\n",
      "(1, 3) (1, 3)\n",
      "(1, 3)\n",
      "(1, 3)\n",
      "(1, 3)\n",
      "Adam: _____ end(1, 3)\n",
      "-------------------------------\n",
      "(5, 3)\n",
      "(5, 3)\n",
      "Adam: _____ start(4, 3)\n",
      "Adam:(4, 3)\n",
      "(4, 3) (4, 3)\n",
      "(4, 3)\n",
      "(4, 3)\n",
      "(4, 3)\n",
      "Adam: _____ end(4, 3)\n",
      "Adam: _____ start(1, 3)\n",
      "Adam:(1, 3)\n",
      "(1, 3) (1, 3)\n",
      "(1, 3)\n",
      "(1, 3)\n",
      "(1, 3)\n",
      "Adam: _____ end(1, 3)\n",
      "-------------------------------\n",
      "(5, 3)\n",
      "(5, 3)\n",
      "Adam: _____ start(4, 3)\n",
      "Adam:(4, 3)\n",
      "(4, 3) (4, 3)\n",
      "(4, 3)\n",
      "(4, 3)\n",
      "(4, 3)\n",
      "Adam: _____ end(4, 3)\n",
      "Adam: _____ start(1, 3)\n",
      "Adam:(1, 3)\n",
      "(1, 3) (1, 3)\n",
      "(1, 3)\n",
      "(1, 3)\n",
      "(1, 3)\n",
      "Adam: _____ end(1, 3)\n",
      "-------------------------------\n",
      "(5, 3)\n",
      "(5, 3)\n",
      "Adam: _____ start(4, 3)\n",
      "Adam:(4, 3)\n",
      "(4, 3) (4, 3)\n",
      "(4, 3)\n",
      "(4, 3)\n",
      "(4, 3)\n",
      "Adam: _____ end(4, 3)\n",
      "Adam: _____ start(1, 3)\n",
      "Adam:(1, 3)\n",
      "(1, 3) (1, 3)\n",
      "(1, 3)\n",
      "(1, 3)\n",
      "(1, 3)\n",
      "Adam: _____ end(1, 3)\n",
      "-------------------------------\n",
      "(5, 3)\n",
      "(5, 3)\n",
      "Adam: _____ start(4, 3)\n",
      "Adam:(4, 3)\n",
      "(4, 3) (4, 3)\n",
      "(4, 3)\n",
      "(4, 3)\n",
      "(4, 3)\n",
      "Adam: _____ end(4, 3)\n",
      "Adam: _____ start(1, 3)\n",
      "Adam:(1, 3)\n",
      "(1, 3) (1, 3)\n",
      "(1, 3)\n",
      "(1, 3)\n",
      "(1, 3)\n",
      "Adam: _____ end(1, 3)\n",
      "-------------------------------\n",
      "(5, 3)\n",
      "(5, 3)\n",
      "Adam: _____ start(4, 3)\n",
      "Adam:(4, 3)\n",
      "(4, 3) (4, 3)\n",
      "(4, 3)\n",
      "(4, 3)\n",
      "(4, 3)\n",
      "Adam: _____ end(4, 3)\n",
      "Adam: _____ start(1, 3)\n",
      "Adam:(1, 3)\n",
      "(1, 3) (1, 3)\n",
      "(1, 3)\n",
      "(1, 3)\n",
      "(1, 3)\n",
      "Adam: _____ end(1, 3)\n",
      "-------------------------------\n",
      "(5, 3)\n",
      "(5, 3)\n",
      "Adam: _____ start(4, 3)\n",
      "Adam:(4, 3)\n",
      "(4, 3) (4, 3)\n",
      "(4, 3)\n",
      "(4, 3)\n",
      "(4, 3)\n",
      "Adam: _____ end(4, 3)\n",
      "Adam: _____ start(1, 3)\n",
      "Adam:(1, 3)\n",
      "(1, 3) (1, 3)\n",
      "(1, 3)\n",
      "(1, 3)\n",
      "(1, 3)\n",
      "Adam: _____ end(1, 3)\n",
      "-------------------------------\n",
      "(5, 3)\n",
      "(5, 3)\n",
      "Adam: _____ start(4, 3)\n",
      "Adam:(4, 3)\n",
      "(4, 3) (4, 3)\n",
      "(4, 3)\n",
      "(4, 3)\n",
      "(4, 3)\n",
      "Adam: _____ end(4, 3)\n",
      "Adam: _____ start(1, 3)\n",
      "Adam:(1, 3)\n",
      "(1, 3) (1, 3)\n",
      "(1, 3)\n",
      "(1, 3)\n",
      "(1, 3)\n",
      "Adam: _____ end(1, 3)\n",
      "-------------------------------\n",
      "(5, 3)\n",
      "(5, 3)\n",
      "Adam: _____ start(4, 3)\n",
      "Adam:(4, 3)\n",
      "(4, 3) (4, 3)\n",
      "(4, 3)\n",
      "(4, 3)\n",
      "(4, 3)\n",
      "Adam: _____ end(4, 3)\n",
      "Adam: _____ start(1, 3)\n",
      "Adam:(1, 3)\n",
      "(1, 3) (1, 3)\n",
      "(1, 3)\n",
      "(1, 3)\n",
      "(1, 3)\n",
      "Adam: _____ end(1, 3)\n",
      "-------------------------------\n",
      "(5, 3)\n",
      "(5, 3)\n",
      "Adam: _____ start(4, 3)\n",
      "Adam:(4, 3)\n",
      "(4, 3) (4, 3)\n",
      "(4, 3)\n",
      "(4, 3)\n",
      "(4, 3)\n",
      "Adam: _____ end(4, 3)\n",
      "Adam: _____ start(1, 3)\n",
      "Adam:(1, 3)\n",
      "(1, 3) (1, 3)\n",
      "(1, 3)\n",
      "(1, 3)\n",
      "(1, 3)\n",
      "Adam: _____ end(1, 3)\n",
      "-------------------------------\n",
      "(5, 3)\n",
      "(5, 3)\n",
      "Adam: _____ start(4, 3)\n",
      "Adam:(4, 3)\n",
      "(4, 3) (4, 3)\n",
      "(4, 3)\n",
      "(4, 3)\n",
      "(4, 3)\n",
      "Adam: _____ end(4, 3)\n",
      "Adam: _____ start(1, 3)\n",
      "Adam:(1, 3)\n",
      "(1, 3) (1, 3)\n",
      "(1, 3)\n",
      "(1, 3)\n",
      "(1, 3)\n",
      "Adam: _____ end(1, 3)\n",
      "-------------------------------\n",
      "(5, 3)\n",
      "(5, 3)\n",
      "Adam: _____ start(4, 3)\n",
      "Adam:(4, 3)\n",
      "(4, 3) (4, 3)\n",
      "(4, 3)\n",
      "(4, 3)\n",
      "(4, 3)\n",
      "Adam: _____ end(4, 3)\n",
      "Adam: _____ start(1, 3)\n",
      "Adam:(1, 3)\n",
      "(1, 3) (1, 3)\n",
      "(1, 3)\n",
      "(1, 3)\n",
      "(1, 3)\n",
      "Adam: _____ end(1, 3)\n",
      "-------------------------------\n",
      "(5, 3)\n",
      "(5, 3)\n",
      "Adam: _____ start(4, 3)\n",
      "Adam:(4, 3)\n",
      "(4, 3) (4, 3)\n",
      "(4, 3)\n",
      "(4, 3)\n",
      "(4, 3)\n",
      "Adam: _____ end(4, 3)\n",
      "Adam: _____ start(1, 3)\n",
      "Adam:(1, 3)\n",
      "(1, 3) (1, 3)\n",
      "(1, 3)\n",
      "(1, 3)\n",
      "(1, 3)\n",
      "Adam: _____ end(1, 3)\n",
      "-------------------------------\n",
      "(5, 3)\n",
      "(5, 3)\n",
      "Adam: _____ start(4, 3)\n",
      "Adam:(4, 3)\n",
      "(4, 3) (4, 3)\n",
      "(4, 3)\n",
      "(4, 3)\n",
      "(4, 3)\n",
      "Adam: _____ end(4, 3)\n",
      "Adam: _____ start(1, 3)\n",
      "Adam:(1, 3)\n",
      "(1, 3) (1, 3)\n",
      "(1, 3)\n",
      "(1, 3)\n",
      "(1, 3)\n",
      "Adam: _____ end(1, 3)\n",
      "-------------------------------\n",
      "(5, 3)\n",
      "(5, 3)\n",
      "Adam: _____ start(4, 3)\n",
      "Adam:(4, 3)\n",
      "(4, 3) (4, 3)\n",
      "(4, 3)\n",
      "(4, 3)\n",
      "(4, 3)\n",
      "Adam: _____ end(4, 3)\n",
      "Adam: _____ start(1, 3)\n",
      "Adam:(1, 3)\n",
      "(1, 3) (1, 3)\n",
      "(1, 3)\n",
      "(1, 3)\n",
      "(1, 3)\n",
      "Adam: _____ end(1, 3)\n",
      "-------------------------------\n",
      "(5, 3)\n",
      "(5, 3)\n",
      "Adam: _____ start(4, 3)\n",
      "Adam:(4, 3)\n",
      "(4, 3) (4, 3)\n",
      "(4, 3)\n",
      "(4, 3)\n",
      "(4, 3)\n",
      "Adam: _____ end(4, 3)\n",
      "Adam: _____ start(1, 3)\n",
      "Adam:(1, 3)\n",
      "(1, 3) (1, 3)\n",
      "(1, 3)\n",
      "(1, 3)\n",
      "(1, 3)\n",
      "Adam: _____ end(1, 3)\n",
      "-------------------------------\n",
      "(5, 3)\n",
      "(5, 3)\n",
      "Adam: _____ start(4, 3)\n",
      "Adam:(4, 3)\n",
      "(4, 3) (4, 3)\n",
      "(4, 3)\n",
      "(4, 3)\n",
      "(4, 3)\n",
      "Adam: _____ end(4, 3)\n",
      "Adam: _____ start(1, 3)\n",
      "Adam:(1, 3)\n",
      "(1, 3) (1, 3)\n",
      "(1, 3)\n",
      "(1, 3)\n",
      "(1, 3)\n",
      "Adam: _____ end(1, 3)\n",
      "-------------------------------\n",
      "(5, 3)\n",
      "(5, 3)\n",
      "Adam: _____ start(4, 3)\n",
      "Adam:(4, 3)\n",
      "(4, 3) (4, 3)\n",
      "(4, 3)\n",
      "(4, 3)\n",
      "(4, 3)\n",
      "Adam: _____ end(4, 3)\n",
      "Adam: _____ start(1, 3)\n",
      "Adam:(1, 3)\n",
      "(1, 3) (1, 3)\n",
      "(1, 3)\n",
      "(1, 3)\n",
      "(1, 3)\n",
      "Adam: _____ end(1, 3)\n",
      "-------------------------------\n",
      "(5, 3)\n",
      "(5, 3)\n",
      "Adam: _____ start(4, 3)\n",
      "Adam:(4, 3)\n",
      "(4, 3) (4, 3)\n",
      "(4, 3)\n",
      "(4, 3)\n",
      "(4, 3)\n",
      "Adam: _____ end(4, 3)\n",
      "Adam: _____ start(1, 3)\n",
      "Adam:(1, 3)\n",
      "(1, 3) (1, 3)\n",
      "(1, 3)\n",
      "(1, 3)\n",
      "(1, 3)\n",
      "Adam: _____ end(1, 3)\n",
      "0.54 0.9\n"
     ]
    }
   ],
   "source": [
    "from Optimization import *\n",
    "from Layers import *\n",
    "def test_iris_data_with_adam():\n",
    "        net = NeuralNetwork(Optimizers.Adam(1e-2, 0.9, 0.999),\n",
    "                                          Initializers.UniformRandom(),\n",
    "                                          Initializers.Constant(0.1))\n",
    "        categories = 3\n",
    "        input_size = 4\n",
    "        net.data_layer = Helpers.IrisData(100)\n",
    "        net.loss_layer = Loss.CrossEntropyLoss()\n",
    "        fcl_1 = FullyConnected.FullyConnected(input_size, categories)\n",
    "        net.append_layer(fcl_1)\n",
    "        net.append_layer(SoftMax.SoftMax())\n",
    "        net.train(20)\n",
    "        data, labels = net.data_layer.get_test_set()\n",
    "\n",
    "        results = net.test(data)\n",
    "        accuracy = Helpers.calculate_accuracy(results, labels)\n",
    "        print(accuracy, 0.9)\n",
    "\n",
    "test_iris_data_with_adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 3)\n",
      "Adam: _____ start(1, 3)\n",
      "Adam:(1, 3)\n",
      "(1, 3)\n",
      "(1, 3)\n",
      "Adam: _____ end(1, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = np.random.random((4, 3))\n",
    "print(arr.shape)\n",
    "a=Optimizers.Adam(5e-3, 0.98, 0.999)\n",
    "a.calculate_update(arr[-1:,:],arr[-1:,:]).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "courseUse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
