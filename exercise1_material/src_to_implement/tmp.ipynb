{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Layers.FullyConnected as fc\n",
    "fn = fc(10,20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 10)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "m102= np.random.uniform(0,1,(7,10))\n",
    "m510 = np.random.uniform(0,1,(5,7))\n",
    "m102.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from Optimization import Optimizers\n",
    "class BaseLayer:\n",
    "    def __init__(self,trainable=False,weight=None):\n",
    "        self.trainable = trainable\n",
    "        self.weights = weight\n",
    "\n",
    " \n",
    "class FullyConnected(BaseLayer):\n",
    "    def __init__(self,input_size,output_size):\n",
    "        super().__init__(True,np.random.uniform(0,1,(input_size+1,output_size))) #weights + bias\n",
    "        self.grad_weights = None\n",
    "        self._optimizer = None\n",
    "    def set_optimizer(self,optimizer):\n",
    "        self._optimizer = optimizer\n",
    "    def get_optimizer(self):\n",
    "        return self._optimizer\n",
    "    def forward(self,input_tensor):\n",
    "        '''\n",
    "            input shape : (bs,input_size) \n",
    "            weights shape :(input_size , output_size)\n",
    "            bias shape:(1,output_size)\n",
    "            input * weight + bias = (bs,input_size)*(input_size,output_size)+(1,output_size)\n",
    "            return (bs,output_size)\n",
    "        '''\n",
    "        self.input_tensor = input_tensor\n",
    "        return np.dot(input_tensor,self.weights[:-1,:])+self.weights[-1:,:]\n",
    "    def backward(self,error_tensor):\n",
    "        '''\n",
    "            &L/&W = self.input_tensor.T(inputsize,bs) * erro_tensor(bs,outsize) =(inputsize,outsize)\n",
    "            &L/&b = error_tensor.sum to (1,outsize)\n",
    "        '''\n",
    "        self.grad_weights = np.dot(self.input_tensor.T , error_tensor)\n",
    "        self.grad_bias = np.sum(error_tensor ,axis=0)\n",
    "        self._optimizer.calculate_update(self.weights[:-1,:],self.grad_weights)\n",
    "        self._optimizer.calculate_update(self.weights[-1:,:],self.grad_bias)\n",
    "f1 = FullyConnected(10,20)\n",
    "f1.optimizer = Optimizers.Sgd(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51.42762299797902\n",
      "51.42762299797902\n"
     ]
    }
   ],
   "source": [
    "batch_size = 9\n",
    "input_size = 4\n",
    "output_size = 3\n",
    "input_tensor = np.random.rand(batch_size, input_size)\n",
    "layer = FullyConnected(input_size, output_size)\n",
    "layer.set_optimizer(Optimizers.Sgd(1))\n",
    "\n",
    "output_tensor = layer.forward(input_tensor)\n",
    "error_tensor = np.zeros([batch_size, output_size])\n",
    "error_tensor -= output_tensor\n",
    "            # print(error_tensor.shape)\n",
    "layer.backward(error_tensor)\n",
    "new_output_tensor = layer.forward(input_tensor)\n",
    "print(np.sum(np.power(output_tensor, 2)))\n",
    "print(np.sum(np.power(new_output_tensor, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.29977113  0.96987881  0.70923767  0.65452177]\n",
      " [ 0.93262138 -1.          0.87956377  0.79573793]]\n",
      "[[ 0.29977113  0.96987881  0.70923767  0.65452177]\n",
      " [ 0.93262138 -0.          0.87956377  0.79573793]]\n"
     ]
    }
   ],
   "source": [
    "c=np.random.random((2,4))\n",
    "c[1,1]=-1\n",
    "print(c)\n",
    "print(c * (c >0).astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax 概率分布:\n",
      " [[0.65900114 0.24243297 0.09856589]\n",
      " [0.14767203 0.73142434 0.12090363]]\n",
      "交叉熵损失值: 0.3648957545487306\n",
      "损失对 logits 的梯度:\n",
      " [[-0.17049943  0.12121649  0.04928295]\n",
      " [ 0.07383601 -0.13428783  0.06045182]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 模拟 logits 和真实标签\n",
    "logits = np.array([[2.0, 1.0, 0.1], [0.5, 2.1, 0.3]])  # (N, C) 形状\n",
    "y_true = np.array([0, 1])  # 真实标签索引 (N,)\n",
    "\n",
    "# One-Hot 编码\n",
    "num_classes = logits.shape[1]\n",
    "y_onehot = np.eye(num_classes)[y_true]\n",
    "\n",
    "# Softmax 前向传播\n",
    "exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))  # 稳定 Softmax\n",
    "softmax = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
    "\n",
    "# 交叉熵损失\n",
    "loss = -np.sum(y_onehot * np.log(softmax)) / logits.shape[0]\n",
    "\n",
    "# 反向传播：损失对 logits 的梯度\n",
    "grad_logits = (softmax - y_onehot) / logits.shape[0]\n",
    "\n",
    "# 输出结果\n",
    "print(\"Softmax 概率分布:\\n\", softmax)\n",
    "print(\"交叉熵损失值:\", loss)\n",
    "print(\"损失对 logits 的梯度:\\n\", grad_logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "categories=4\n",
    "batch_size=9\n",
    "label_tensor = np.zeros([batch_size, categories])\n",
    "print(label_tensor)\n",
    "for i in range(batch_size):\n",
    "    label_tensor[i, np.random.randint(0, categories)] = 1\n",
    "print(label_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1=np.array([1,1,0,0,1])\n",
    "x2=np.array([1,1,1,1,1])\n",
    "x1[x2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 4)\n",
      "(50, 3)\n"
     ]
    }
   ],
   "source": [
    "from Layers import *\n",
    "data_layer=Helpers.IrisData(50)\n",
    "inpu,la = data_layer.next()\n",
    "print(inpu.shape)\n",
    "print(la.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "[[0.95853426 0.52767175 0.34233081 0.23615943 0.95853473 0.69945299\n",
      "  0.04639071 0.9918899  0.68529956 0.45551153]\n",
      " [0.79932413 0.46298366 0.086053   0.45836218 0.66441642 0.95435384\n",
      "  0.12149753 0.9303957  0.35823565 0.69545343]\n",
      " [0.63786152 0.10540237 0.0837914  0.21325733 0.12651872 0.5958658\n",
      "  0.7694232  0.09080234 0.39702149 0.84720904]\n",
      " [0.50257649 0.85657757 0.77369515 0.15300915 0.70269489 0.1470951\n",
      "  0.7802047  0.01999168 0.83133624 0.86435769]\n",
      " [0.48247934 0.63253051 0.62734556 0.15544277 0.88046778 0.15380405\n",
      "  0.77569244 0.8935717  0.24370536 0.63791389]]\n"
     ]
    }
   ],
   "source": [
    "from Layers.Base import BaseLayer\n",
    "import numpy as np\n",
    "b1 = BaseLayer(True, np.random.uniform(0, 1, (5, 10)))\n",
    "print(b1.trainable)\n",
    "print(b1.weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "courseUse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
